{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model\n",
    "from sklearn import ensemble\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here I am creating so helper functions to help with parsing out our text\n",
    "def clean_soup(soup):\n",
    "    for tag in soup.find_all(['script', 'style','meta']):\n",
    "        tag.decompose()   \n",
    "    return soup.get_text()\n",
    "\n",
    "\n",
    "def find_between( s, first, last ):\n",
    "    try:\n",
    "        start = s.index( first ) + len( first )\n",
    "        end = s.index( last, start )\n",
    "        return s[start:end]\n",
    "    except ValueError:\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roland/anaconda3/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /home/roland/anaconda3/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# This is the hard coded directory in which the raw html lives on my computer\n",
    "rootdir = '/home/roland/Workspace/Data/Procrastinating_HTML/'\n",
    "# Here we are creating a data frame to store the classified information in\n",
    "data = pd.DataFrame(columns = ['activity', 'text', 'timestamp'])\n",
    "\n",
    "# These strings were placed into the HTML by my browser extension to hold onto store the url\n",
    "URL_STRING = \"__URL__: \"\n",
    "HTML_START = \"<\"\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    i = 0\n",
    "    for file in files:\n",
    "        f = open(os.path.join(subdir, file))\n",
    "        raw = f.read()\n",
    "        url = ''\n",
    "        # Parsing out the URL string if it exists\n",
    "        if URL_STRING in raw:\n",
    "            url = find_between(raw, URL_STRING, HTML_START)\n",
    "            raw = raw.replace(url, '')\n",
    "        \n",
    "        # Below are characters I am manually parsing out. There must exist a more efficient way to do this\n",
    "        # but for now it runs fast enough.\n",
    "        raw = raw.replace('\\n', ' ')\n",
    "        raw = raw.replace('\\t', ' ')\n",
    "        raw = raw.replace('\\\\n', ' ')\n",
    "        raw = raw.replace('\\\\t', ' ')\n",
    "        raw = raw.replace(URL_STRING, '')\n",
    "\n",
    "        soup = BeautifulSoup(raw)\n",
    "        data.set_value(i, 'text', clean_soup(soup))\n",
    "        t = str(file).split('_')\n",
    "        data.set_value(i, 'activity', t[0])\n",
    "        timestamp = t[1].split('.')[0]\n",
    "        data.set_value(i, 'timestamp', timestamp)\n",
    "        data.set_value(i, 'url', url)\n",
    "        i+=1\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data[data.text != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.timestamp = data.timestamp.str.replace(\"\\(1\\)\",\"\")\n",
    "data.timestamp = data.timestamp.str.replace(\"T\",\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data[['text','url']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(data, data.activity,test_size=0.33, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a grid searched pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using a grid search over a pipeline to find the best model to use. After much trial and error I settled on a logistic regression model. First off it simply performaned the best. But additionally it is by far the most interpretible model. Also, Lasso Regularization is a great way to reduce the number of features generated by the countvectorization process, and the use of ngrams. At one point I had a dataframe with over 2,000,000 features. Lasso Regularization reduced that to a few hundred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97493036211699169"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "\n",
    "# Let's create some stop words. I chose these values after doing a little bit of EDA.\n",
    "stop = stopwords.words('english')\n",
    "stop = stop + ['https', 'www', 'com', 'http']\n",
    "cvt = CountVectorizer(stop_words=stop, ngram_range=[1,4])\n",
    "\n",
    "# Here we are initializing the values we want to grid search over.\n",
    "param_grid = dict(vect = [CountVectorizer()],\n",
    "                  vect__ngram_range=[[1,3],[1,4]], # Trying different ngram ranges\n",
    "                  vect__stop_words = [stop],\n",
    "                tfidf = [TfidfTransformer()],\n",
    "                tfidf__norm = [None],\n",
    "                clf=[LogisticRegression()],\n",
    "                clf__C=[.04,.1,.06, .07, .05], # Trying different coefficients for alpha\n",
    "                clf__penalty=['l1'])\n",
    "            \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', cvt),\n",
    "    ('tfidf', TfidfTransformer(norm=None)),\n",
    "    ('clf', LogisticRegression(penalty='l1'))\n",
    "]) \n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid)\n",
    "\n",
    "grid_search.fit(X_train.text, y_train)\n",
    "grid_search.best_estimator_.score(X_test.text, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing our results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy score is looking great. 97.5% is great, but we should compare it to our baseline distribution before we get to excited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41483516483516486"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Caluclating our baseline\n",
    "(y_train == 'work').sum()/float(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have massively improved over random chance. This is a good start. We should look at some additional metrics as well to see if we have anything to be concerned about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding our best pipeline and pulling out the useful components\n",
    "pipeline = grid_search.best_estimator_\n",
    "lm =  pipeline.named_steps['clf']\n",
    "vect = pipeline.named_steps['vect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This bit of code is pulling out my features that have coefficients greater than zero\n",
    "# Lasso regularization reduces the coef 0 of the features (in our case unique ngrams)\n",
    "import math\n",
    "features =(vect.get_feature_names())\n",
    "feature_dict = {}\n",
    "for (i, f) in enumerate(features):\n",
    "    if np.abs(lm.coef_[0][i]) > 0:\n",
    "        feature_dict[f] = lm.coef_[0][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame.from_dict(feature_dict, orient='index')\n",
    "feature_df.columns = ['coef']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look oat our number of features and our total documents. We really do not want a model utilizing more features than we have documents. Our reguralization should have accounted for this, but it's not a bad idea to double check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs: 728\n",
      "Number of features: 168\n"
     ]
    }
   ],
   "source": [
    "# How many documents do I have in my training set\n",
    "print(\"Number of docs: \" + str(len(X_train)))\n",
    "\n",
    "# How many features do I have after reguralization\n",
    "print(\"Number of features: \" + str(len(feature_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can raise or logisitic regression coef to the e to calculate the odds ratio\n",
    "feature_df['odds_ratio'] = feature_df['coef'].apply(np.exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at what words are most associated with procrastination and productivity. We can sort our dataframe by odds ratio. The smaller odds ratio means words that are less related productivity, and a higher ratio means more related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>odds_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>game</th>\n",
       "      <td>-0.137337</td>\n",
       "      <td>0.871676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>likes</th>\n",
       "      <td>-0.082925</td>\n",
       "      <td>0.920420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reddit</th>\n",
       "      <td>-0.078881</td>\n",
       "      <td>0.924150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>photo</th>\n",
       "      <td>-0.063171</td>\n",
       "      <td>0.938783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>attack</th>\n",
       "      <td>-0.057691</td>\n",
       "      <td>0.943942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>src</th>\n",
       "      <td>-0.056463</td>\n",
       "      <td>0.945101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thwas</th>\n",
       "      <td>-0.055762</td>\n",
       "      <td>0.945764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video</th>\n",
       "      <td>-0.042215</td>\n",
       "      <td>0.958664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>us</th>\n",
       "      <td>-0.041966</td>\n",
       "      <td>0.958902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5e</th>\n",
       "      <td>-0.038311</td>\n",
       "      <td>0.962414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            coef  odds_ratio\n",
       "game   -0.137337    0.871676\n",
       "likes  -0.082925    0.920420\n",
       "reddit -0.078881    0.924150\n",
       "photo  -0.063171    0.938783\n",
       "attack -0.057691    0.943942\n",
       "src    -0.056463    0.945101\n",
       "thwas  -0.055762    0.945764\n",
       "video  -0.042215    0.958664\n",
       "us     -0.041966    0.958902\n",
       "5e     -0.038311    0.962414"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.sort_values('odds_ratio').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>odds_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>github</th>\n",
       "      <td>0.170815</td>\n",
       "      <td>1.186271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>using</th>\n",
       "      <td>0.157641</td>\n",
       "      <td>1.170745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data</th>\n",
       "      <td>0.115566</td>\n",
       "      <td>1.122509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file</th>\n",
       "      <td>0.071850</td>\n",
       "      <td>1.074494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>code</th>\n",
       "      <td>0.068924</td>\n",
       "      <td>1.071355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>import</th>\n",
       "      <td>0.063022</td>\n",
       "      <td>1.065050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>instagram</th>\n",
       "      <td>0.053534</td>\n",
       "      <td>1.054993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>friction</th>\n",
       "      <td>0.052766</td>\n",
       "      <td>1.054183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stack</th>\n",
       "      <td>0.051788</td>\n",
       "      <td>1.053153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kurzgesagt</th>\n",
       "      <td>0.039332</td>\n",
       "      <td>1.040116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                coef  odds_ratio\n",
       "github      0.170815    1.186271\n",
       "using       0.157641    1.170745\n",
       "data        0.115566    1.122509\n",
       "file        0.071850    1.074494\n",
       "code        0.068924    1.071355\n",
       "import      0.063022    1.065050\n",
       "instagram   0.053534    1.054993\n",
       "friction    0.052766    1.054183\n",
       "stack       0.051788    1.053153\n",
       "kurzgesagt  0.039332    1.040116"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.sort_values('odds_ratio', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_x =vect.transform(X_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roland/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "X_test['prob_work'] = lm.predict_proba(result_x)[:,1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roland/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/roland/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_test['prob_work'] = X_test.prob_work.apply(int)\n",
    "X_test['predict']  = lm.predict(result_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity</th>\n",
       "      <th>text</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>url</th>\n",
       "      <th>prob_work</th>\n",
       "      <th>predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>818</th>\n",
       "      <td>procr</td>\n",
       "      <td>AnyDice            AnyDice Dice Probability C...</td>\n",
       "      <td>2017-7-27-11-0-51</td>\n",
       "      <td></td>\n",
       "      <td>57</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>work</td>\n",
       "      <td>K-means Clustering : pystatsjump to contentMy ...</td>\n",
       "      <td>2017-7-25-19-59-3</td>\n",
       "      <td></td>\n",
       "      <td>42</td>\n",
       "      <td>procr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>procr</td>\n",
       "      <td>dnd 5e - Multiclass Warlock/Wizard: Can I use...</td>\n",
       "      <td>2017-7-24-10-22-3</td>\n",
       "      <td></td>\n",
       "      <td>75</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>procr</td>\n",
       "      <td>[OC] snapchat heat map aligning with path of E...</td>\n",
       "      <td>2017-8-21-15-13-0</td>\n",
       "      <td>https://www.reddit.com/r/dataisbeautiful/comme...</td>\n",
       "      <td>63</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>procr</td>\n",
       "      <td>Python (genus) - Wikipedia                  ...</td>\n",
       "      <td>2017-8-5-3-23-47</td>\n",
       "      <td></td>\n",
       "      <td>72</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>work</td>\n",
       "      <td>Dashboard                               ...</td>\n",
       "      <td>2017-8-22-14-40-40</td>\n",
       "      <td>https://git.generalassemb.ly/orgs/DSI-DC-5/das...</td>\n",
       "      <td>48</td>\n",
       "      <td>procr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>work</td>\n",
       "      <td>Balances                                      ...</td>\n",
       "      <td>2017-8-22-14-9-28</td>\n",
       "      <td>https://ebranch.nasafcu.com/HBNet/App/Account/...</td>\n",
       "      <td>48</td>\n",
       "      <td>procr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>work</td>\n",
       "      <td>Python-written open source tool to transform e...</td>\n",
       "      <td>2017-7-25-19-52-37</td>\n",
       "      <td></td>\n",
       "      <td>37</td>\n",
       "      <td>procr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>procr</td>\n",
       "      <td>Dead-Baby Jokes               about |  contac...</td>\n",
       "      <td>2017-8-18-12-34-50</td>\n",
       "      <td>http://www.skrause.org/humor/deadbaby.shtml</td>\n",
       "      <td>50</td>\n",
       "      <td>work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>work</td>\n",
       "      <td>(1) Intro to vectors &amp; scalars | One-di...</td>\n",
       "      <td>2017-7-20-18-20-21</td>\n",
       "      <td></td>\n",
       "      <td>34</td>\n",
       "      <td>procr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>work</td>\n",
       "      <td>Derangements - Numberphile - YouTube  ...</td>\n",
       "      <td>2017-8-2-11-29-40</td>\n",
       "      <td></td>\n",
       "      <td>41</td>\n",
       "      <td>procr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>work</td>\n",
       "      <td>America's Pastimejump to contentMy Subreddits-...</td>\n",
       "      <td>2017-7-25-0-25-38</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>procr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    activity                                               text  \\\n",
       "818    procr   AnyDice            AnyDice Dice Probability C...   \n",
       "147     work  K-means Clustering : pystatsjump to contentMy ...   \n",
       "495    procr   dnd 5e - Multiclass Warlock/Wizard: Can I use...   \n",
       "997    procr  [OC] snapchat heat map aligning with path of E...   \n",
       "389    procr    Python (genus) - Wikipedia                  ...   \n",
       "505     work        Dashboard                               ...   \n",
       "427     work  Balances                                      ...   \n",
       "855     work  Python-written open source tool to transform e...   \n",
       "860    procr   Dead-Baby Jokes               about |  contac...   \n",
       "806     work         (1) Intro to vectors & scalars | One-di...   \n",
       "625     work          Derangements - Numberphile - YouTube  ...   \n",
       "611     work  America's Pastimejump to contentMy Subreddits-...   \n",
       "\n",
       "              timestamp                                                url  \\\n",
       "818   2017-7-27-11-0-51                                                      \n",
       "147   2017-7-25-19-59-3                                                      \n",
       "495   2017-7-24-10-22-3                                                      \n",
       "997   2017-8-21-15-13-0  https://www.reddit.com/r/dataisbeautiful/comme...   \n",
       "389    2017-8-5-3-23-47                                                      \n",
       "505  2017-8-22-14-40-40  https://git.generalassemb.ly/orgs/DSI-DC-5/das...   \n",
       "427   2017-8-22-14-9-28  https://ebranch.nasafcu.com/HBNet/App/Account/...   \n",
       "855  2017-7-25-19-52-37                                                      \n",
       "860  2017-8-18-12-34-50        http://www.skrause.org/humor/deadbaby.shtml   \n",
       "806  2017-7-20-18-20-21                                                      \n",
       "625   2017-8-2-11-29-40                                                      \n",
       "611   2017-7-25-0-25-38                                                      \n",
       "\n",
       "     prob_work predict  \n",
       "818         57    work  \n",
       "147         42   procr  \n",
       "495         75    work  \n",
       "997         63    work  \n",
       "389         72    work  \n",
       "505         48   procr  \n",
       "427         48   procr  \n",
       "855         37   procr  \n",
       "860         50    work  \n",
       "806         34   procr  \n",
       "625         41   procr  \n",
       "611          7   procr  "
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[X_test.activity != X_test.predict]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results from my initial run through have been extremely successful. I have a f7unctioning model with above a 95% accuracy. My base line is about 60% so this is a large improvement. I have used logistic regression with regularization to get my intial results, but I have some concerns about that approach.\n",
    "\n",
    "I currently have 105000 features wiht non-zero coefficients. This is vastly more than the number of documents I have in my training set (~700). I have discussed several approaches to solving this issue. One, is that my model is surviving a train test split. It is accurate, despite having the potential to be over fitted. I think the reguralization is doing it's part. Nonetheless, I am going to try to reduce the number of features and see how much my accuracy is actually impacted.\n",
    "\n",
    "Secondly I want to try different modeling methods. It has been suggested that I use and SVM. I will also try a random forest, given that they handle large numbers of features well.\n",
    "\n",
    "Finally, I would like to do some more exploration of what features are correlated with what URLs. I have some artificat features that don't correlate with real words. I want to know if they only show up on certain websites, or if they exist accross multiple websites.\n",
    "\n",
    "All in all, my model is in good shape. I have a strong predictability. I will spend some time doing additional exploration, but overall I think I am in good shape. I need to make some additional visualizations for my presentation (I have several in mind)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
